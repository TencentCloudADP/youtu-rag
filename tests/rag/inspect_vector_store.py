"""
æ£€æŸ¥ ChromaDB å‘é‡å­˜å‚¨çš„è¯¦ç»†ä¿¡æ¯

åŠŸèƒ½ï¼š
1. åˆ—å‡ºæ‰€æœ‰ collection
2. æ˜¾ç¤ºæ¯ä¸ª collection çš„ç»Ÿè®¡ä¿¡æ¯
3. æŸ¥çœ‹æ–‡æ¡£çš„å…ƒæ•°æ®è¯¦æƒ…
4. æ£€æŸ¥å…ƒæ•°æ®å­—æ®µçš„åˆ†å¸ƒæƒ…å†µ

ä½¿ç”¨æ–¹æ³•ï¼š
    uv run python tests/rag/inspect_vector_store.py
    uv run python tests/rag/inspect_vector_store.py --collection kb_1_20251207_145519
    uv run python tests/rag/inspect_vector_store.py --show-samples 5
"""

import argparse
import json
import sys
from pathlib import Path
from collections import Counter, defaultdict

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import chromadb


def format_size(num_bytes: int) -> str:
    """æ ¼å¼åŒ–å­—èŠ‚å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if num_bytes < 1024.0:
            return f"{num_bytes:.2f} {unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.2f} TB"


def analyze_metadata_fields(metadatas: list[dict]) -> dict:
    """åˆ†æå…ƒæ•°æ®å­—æ®µåˆ†å¸ƒ"""
    field_counter = Counter()
    field_values = defaultdict(set)

    for metadata in metadatas:
        if metadata:
            for key, value in metadata.items():
                field_counter[key] += 1
                # æ”¶é›†å”¯ä¸€å€¼ï¼ˆé™åˆ¶æ¯ä¸ªå­—æ®µæœ€å¤šæ”¶é›†10ä¸ªç¤ºä¾‹ï¼‰
                if len(field_values[key]) < 10:
                    field_values[key].add(str(value)[:100] if value else 'None')

    return {
        'field_counts': dict(field_counter),
        'field_samples': {k: list(v) for k, v in field_values.items()}
    }


def inspect_collection(client: chromadb.Client, collection_name: str, show_samples: int = 0):
    """æ£€æŸ¥å•ä¸ª collection çš„è¯¦ç»†ä¿¡æ¯"""
    print(f"\n{'=' * 80}")
    print(f"Collection: {collection_name}")
    print(f"{'=' * 80}")

    try:
        collection = client.get_collection(name=collection_name)

        # è·å– collection åŸºæœ¬ä¿¡æ¯
        count = collection.count()
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  â€¢ æ€»æ–‡æ¡£æ•°: {count:,}")

        if count == 0:
            print("  âš ï¸  Collection ä¸ºç©º")
            return

        # è·å–æ‰€æœ‰æ•°æ®ï¼ˆåˆ†é¡µå¤„ç†å¤§æ•°æ®é›†ï¼‰
        batch_size = 1000
        all_data = {
            'ids': [],
            'metadatas': [],
            'documents': [],
        }

        for offset in range(0, count, batch_size):
            limit = min(batch_size, count - offset)
            result = collection.get(
                limit=limit,
                offset=offset,
                include=['metadatas', 'documents']
            )
            all_data['ids'].extend(result['ids'])
            all_data['metadatas'].extend(result['metadatas'])
            all_data['documents'].extend(result['documents'])

        # åˆ†æå…ƒæ•°æ®
        print(f"\nğŸ“‹ å…ƒæ•°æ®åˆ†æ:")
        metadata_analysis = analyze_metadata_fields(all_data['metadatas'])

        print(f"\n  å­—æ®µç»Ÿè®¡:")
        for field, count in sorted(metadata_analysis['field_counts'].items(), key=lambda x: -x[1]):
            coverage = (count / len(all_data['metadatas'])) * 100
            print(f"    â€¢ {field}: {count:,} ä¸ªæ–‡æ¡£ ({coverage:.1f}% è¦†ç›–ç‡)")

        print(f"\n  å­—æ®µå€¼ç¤ºä¾‹:")
        for field, samples in metadata_analysis['field_samples'].items():
            print(f"    â€¢ {field}:")
            for sample in samples[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªç¤ºä¾‹
                print(f"      - {sample}")

        # ç»Ÿè®¡æ–‡æ¡£é•¿åº¦
        doc_lengths = [len(doc) if doc else 0 for doc in all_data['documents']]
        if doc_lengths:
            print(f"\nğŸ“„ æ–‡æ¡£å†…å®¹ç»Ÿè®¡:")
            print(f"  â€¢ å¹³å‡é•¿åº¦: {sum(doc_lengths) / len(doc_lengths):.0f} å­—ç¬¦")
            print(f"  â€¢ æœ€çŸ­: {min(doc_lengths):,} å­—ç¬¦")
            print(f"  â€¢ æœ€é•¿: {max(doc_lengths):,} å­—ç¬¦")

        # æ£€æŸ¥ç‰¹æ®Šå…ƒæ•°æ®å­—æ®µ
        print(f"\nğŸ” ç‰¹æ®Šå­—æ®µæ£€æŸ¥:")

        # ä½¿ç”¨å®é™…è·å–åˆ°çš„æ–‡æ¡£æ•°é‡
        actual_count = len(all_data['metadatas'])

        # æ£€æŸ¥ ETAG
        etag_count = sum(1 for m in all_data['metadatas'] if m and 'etag' in m)
        print(f"  â€¢ åŒ…å« ETAG: {etag_count}/{actual_count} ({etag_count/actual_count*100:.1f}%)")

        # æ£€æŸ¥å…ƒæ•°æ®å“ˆå¸Œï¼ˆä¸åº”è¯¥å­˜åœ¨ï¼‰
        metadata_hash_count = sum(1 for m in all_data['metadatas'] if m and '_metadata_hash' in m)
        if metadata_hash_count > 0:
            print(f"  âš ï¸  åŒ…å« _metadata_hash (åº”è¯¥ç§»é™¤): {metadata_hash_count}/{actual_count}")
        else:
            print(f"  âœ“ æ—  _metadata_hash å­—æ®µ (æ­£ç¡®)")

        # æ£€æŸ¥æ ‡å‡†å­—æ®µ
        standard_fields = ['char_length', 'publish_date', 'key_timepoints', 'summary']
        for field in standard_fields:
            field_count = sum(1 for m in all_data['metadatas'] if m and field in m)
            if field_count > 0:
                print(f"  â€¢ {field}: {field_count}/{actual_count} ({field_count/actual_count*100:.1f}%)")

        # æŒ‰æ¥æºæ–‡ä»¶åˆ†ç»„
        print(f"\nğŸ“ æ¥æºæ–‡ä»¶ç»Ÿè®¡:")
        source_counter = Counter()
        for metadata in all_data['metadatas']:
            if metadata and 'source' in metadata:
                source_counter[metadata['source']] += 1

        for source, count in source_counter.most_common(20):  # æ˜¾ç¤ºå‰20ä¸ª
            print(f"  â€¢ {source}: {count:,} chunks")

        if len(source_counter) > 20:
            print(f"  ... ä»¥åŠå…¶ä»– {len(source_counter) - 20} ä¸ªæ–‡ä»¶")

        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®
        if show_samples > 0:
            print(f"\nğŸ“ æ ·æœ¬æ•°æ® (å‰ {show_samples} æ¡):")
            actual_count = len(all_data['ids'])
            for i in range(min(show_samples, actual_count)):
                try:
                    print(f"\n  --- Sample {i + 1} ---")
                    print(f"  ID: {all_data['ids'][i]}")
                    print(f"  Metadata: {json.dumps(all_data['metadatas'][i], ensure_ascii=False, indent=2)}")
                    doc_preview = all_data['documents'][i][:200] if all_data['documents'][i] else 'None'
                    print(f"  Document (preview): {doc_preview}...")
                except Exception as e:
                    print(f"  ERROR in sample {i + 1}: {e}")
                    import traceback
                    traceback.print_exc()

    except Exception as e:
        print(f"  âœ— é”™è¯¯: {str(e)}")


def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥ ChromaDB å‘é‡å­˜å‚¨è¯¦æƒ…')
    parser.add_argument('--collection', type=str, help='æŒ‡å®šè¦æ£€æŸ¥çš„ collection åç§°')
    parser.add_argument('--show-samples', type=int, default=0, help='æ˜¾ç¤ºæ ·æœ¬æ•°æ®çš„æ•°é‡')
    parser.add_argument('--path', type=str,
                       default='./rag_data/vector_store',
                       help='ChromaDB å­˜å‚¨è·¯å¾„')
    args = parser.parse_args()

    print("=" * 80)
    print("ChromaDB å‘é‡å­˜å‚¨æ£€æŸ¥å·¥å…·")
    print("=" * 80)

    # è¿æ¥åˆ° ChromaDB
    persist_directory = Path(args.path)
    if not persist_directory.exists():
        print(f"\nâœ— é”™è¯¯: å‘é‡å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {persist_directory}")
        return

    print(f"\nğŸ“‚ å­˜å‚¨è·¯å¾„: {persist_directory.absolute()}")

    # è®¡ç®—ç›®å½•å¤§å°
    total_size = sum(f.stat().st_size for f in persist_directory.rglob('*') if f.is_file())
    print(f"ğŸ’¾ æ€»å¤§å°: {format_size(total_size)}")

    # åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯
    try:
        client = chromadb.PersistentClient(path=str(persist_directory))
    except Exception as e:
        print(f"\nâœ— é”™è¯¯: æ— æ³•è¿æ¥åˆ° ChromaDB: {str(e)}")
        return

    # è·å–æ‰€æœ‰ collections
    collections = client.list_collections()
    print(f"\nğŸ“š Collections æ€»æ•°: {len(collections)}")

    if not collections:
        print("  âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½• collection")
        return

    # å¦‚æœæŒ‡å®šäº† collectionï¼Œåªæ£€æŸ¥è¯¥ collection
    if args.collection:
        inspect_collection(client, args.collection, args.show_samples)
    else:
        # åˆ—å‡ºæ‰€æœ‰ collections
        print(f"\nğŸ“‹ Collection åˆ—è¡¨:")
        collection_stats = []

        for collection in collections:
            try:
                count = collection.count()
                collection_stats.append((collection.name, count))
                print(f"  â€¢ {collection.name}: {count:,} documents")
            except Exception as e:
                print(f"  â€¢ {collection.name}: âœ— é”™è¯¯ ({str(e)})")

        # è¯¦ç»†æ£€æŸ¥æ¯ä¸ª collection
        for collection_name, count in collection_stats:
            inspect_collection(client, collection_name, args.show_samples)

    print(f"\n{'=' * 80}")
    print("æ£€æŸ¥å®Œæˆï¼")
    print(f"{'=' * 80}\n")


if __name__ == "__main__":
    main()
